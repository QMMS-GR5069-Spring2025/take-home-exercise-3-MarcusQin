{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f520e66-0982-459c-9ed6-7236e6883a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment set: /Users/lq2242@columbia.edu/F1_Lap_Time_Prediction\nLoading data from S3...\nData loaded successfully\nStarting data preprocessing...\nMerged data shape: (551742, 15)\nSelected 6 categorical features: ['driverRef', 'code', 'forename', 'surname', 'nationality', 'name']\nSelected 5 numerical features: ['lap', 'position', 'year', 'round', 'avg_pitstop_time']\nPerforming one-hot encoding...\nFinal dataset - X shape: (551742, 575), y shape: (551742,)\nReducing dataset size from 551742 to 50000 rows for faster training\nTraining set: (40000, 575), Test set: (10000, 575)\n\n=== Starting MLflow experiment runs ===\n\n\nStarting Run 1 with parameters:\n  max_depth: 5\n  n_estimators: 100\n  min_samples_split: 2\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 07:41:16 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 1\n  Metrics - RMSE: 62915.14, MAE: 12849.33, R2: -0.0246\n--------------------------------------------------\n\nStarting Run 2 with parameters:\n  max_depth: 10\n  n_estimators: 300\n  min_samples_split: 2\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 07:43:10 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 2\n  Metrics - RMSE: 63722.23, MAE: 11564.90, R2: -0.0511\n--------------------------------------------------\n\nStarting Run 3 with parameters:\n  max_depth: 15\n  n_estimators: 500\n  min_samples_split: 2\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 07:47:36 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 3\n  Metrics - RMSE: 70595.26, MAE: 10199.92, R2: -0.2901\n--------------------------------------------------\n\nStarting Run 4 with parameters:\n  max_depth: 20\n  n_estimators: 800\n  min_samples_split: 2\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 07:56:03 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 4\n  Metrics - RMSE: 72878.58, MAE: 8882.09, R2: -0.3749\n--------------------------------------------------\n\nStarting Run 5 with parameters:\n  max_depth: None\n  n_estimators: 1000\n  min_samples_split: 2\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 08:11:40 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 5\n  Metrics - RMSE: 72881.38, MAE: 7774.76, R2: -0.3750\n--------------------------------------------------\n\nStarting Run 6 with parameters:\n  max_depth: 10\n  n_estimators: 100\n  min_samples_split: 5\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 08:12:40 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 6\n  Metrics - RMSE: 62929.33, MAE: 11700.02, R2: -0.0251\n--------------------------------------------------\n\nStarting Run 7 with parameters:\n  max_depth: 10\n  n_estimators: 300\n  min_samples_split: 10\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 08:14:38 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 7\n  Metrics - RMSE: 61867.21, MAE: 11593.31, R2: 0.0092\n--------------------------------------------------\n\nStarting Run 8 with parameters:\n  max_depth: 15\n  n_estimators: 300\n  min_samples_split: 5\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 08:17:32 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 8\n  Metrics - RMSE: 65789.47, MAE: 10166.97, R2: -0.1204\n--------------------------------------------------\n\nStarting Run 9 with parameters:\n  max_depth: 20\n  n_estimators: 500\n  min_samples_split: 5\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 08:23:21 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-columbia-nyc.s3.us-east-1.amazonaws.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 9\n  Metrics - RMSE: 67664.82, MAE: 8932.54, R2: -0.1852\n--------------------------------------------------\n\nStarting Run 10 with parameters:\n  max_depth: 15\n  n_estimators: 800\n  min_samples_split: 10\n  Training model...\n  Evaluating model...\n  Creating artifacts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 08:30:50 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.11.4/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed Run 10\n  Metrics - RMSE: 63459.56, MAE: 10142.09, R2: -0.0425\n--------------------------------------------------\n\n=== Summary of All Runs ===\n\n   run_id  max_depth  n_estimators  ...          rmse           mae        r2\n6       6       10.0           300  ...  61867.206947  11593.305768  0.009206\n0       0        5.0           100  ...  62915.144122  12849.331374 -0.024643\n5       5       10.0           100  ...  62929.332190  11700.022640 -0.025105\n9       9       15.0           800  ...  63459.556434  10142.085339 -0.042452\n1       1       10.0           300  ...  63722.227824  11564.901245 -0.051100\n7       7       15.0           300  ...  65789.466408  10166.966489 -0.120405\n8       8       20.0           500  ...  67664.821977   8932.536956 -0.185190\n2       2       15.0           500  ...  70595.262224  10199.924922 -0.290070\n3       3       20.0           800  ...  72878.578314   8882.089589 -0.374871\n4       4        NaN          1000  ...  72881.380441   7774.757041 -0.374977\n\n[10 rows x 7 columns]\n\n=== All experiment runs completed successfully ===\nYou can now view the results in the MLflow UI\nRemember to take screenshots of:\n1. The MLflow homepage showing all your runs\n2. The detailed page of your best run\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, min, max, stddev\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Set up Spark Session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1_MLflow_Homework\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow optimization for faster Spark to Pandas conversion\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Get Databricks username for experiment path\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/F1_Lap_Time_Prediction\"\n",
    "\n",
    "# Set MLflow experiment with proper path format for Databricks\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"MLflow experiment set: {experiment_path}\")\n",
    "\n",
    "# Load data - only select needed columns to reduce data transfer\n",
    "print(\"Loading data from S3...\")\n",
    "df_laptimes = spark.read.csv('s3://columbia-gr5069-main/raw/lap_times.csv', header=True)\n",
    "df_drivers = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header=True)\n",
    "df_pitstops = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', header=True)\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header=True)\n",
    "df_races = spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header=True)\n",
    "\n",
    "# Convert to Pandas for ease of use with scikit-learn\n",
    "laptimes_pd = df_laptimes.toPandas()\n",
    "drivers_pd = df_drivers.toPandas()\n",
    "pitstops_pd = df_pitstops.toPandas()\n",
    "results_pd = df_results.toPandas()\n",
    "races_pd = df_races.toPandas()\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "\n",
    "# Data preprocessing - only keep needed columns to improve performance\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Pre-filter dataframes to include only needed columns before merging\n",
    "drivers_keep_cols = ['driverId', 'driverRef', 'code', 'forename', 'surname', 'nationality']\n",
    "drivers_pd = drivers_pd[[col for col in drivers_keep_cols if col in drivers_pd.columns]]\n",
    "\n",
    "races_keep_cols = ['raceId', 'name', 'year', 'round', 'location', 'country']\n",
    "races_pd = races_pd[[col for col in races_keep_cols if col in races_pd.columns]]\n",
    "\n",
    "pitstops_keep_cols = ['raceId', 'driverId', 'milliseconds']\n",
    "pitstops_pd = pitstops_pd[[col for col in pitstops_keep_cols if col in pitstops_pd.columns]]\n",
    "\n",
    "# Join laptimes with drivers information\n",
    "merged_data = laptimes_pd.merge(drivers_pd, on='driverId', how='left')\n",
    "# Join with race information\n",
    "merged_data = merged_data.merge(races_pd, on='raceId', how='left')\n",
    "\n",
    "# Add average pit stop time per driver per race\n",
    "avg_pitstops = pitstops_pd.groupby(['raceId', 'driverId']).agg({'milliseconds': 'mean'}).reset_index()\n",
    "avg_pitstops.rename(columns={'milliseconds': 'avg_pitstop_time'}, inplace=True)\n",
    "merged_data = merged_data.merge(avg_pitstops, on=['raceId', 'driverId'], how='left')\n",
    "\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = []\n",
    "for col in ['driverRef', 'code', 'forename', 'surname', 'nationality', 'name', 'location', 'country']:\n",
    "    if col in merged_data.columns:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "numerical_features = []\n",
    "for col in ['lap', 'position', 'year', 'round', 'grid', 'altitude']:\n",
    "    if col in merged_data.columns:\n",
    "        numerical_features.append(col)\n",
    "        merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
    "\n",
    "# Add pit stop time if available\n",
    "if 'avg_pitstop_time' in merged_data.columns:\n",
    "    merged_data['avg_pitstop_time'] = merged_data['avg_pitstop_time'].fillna(merged_data['avg_pitstop_time'].mean())\n",
    "    numerical_features.append('avg_pitstop_time')\n",
    "\n",
    "print(f\"Selected {len(categorical_features)} categorical features: {categorical_features}\")\n",
    "print(f\"Selected {len(numerical_features)} numerical features: {numerical_features}\")\n",
    "\n",
    "# Handle categorical features - one-hot encoding\n",
    "print(\"Performing one-hot encoding...\")\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_features = encoder.fit_transform(merged_data[categorical_features].fillna('Unknown'))\n",
    "\n",
    "# Create a new DataFrame with string column names for the encoded features\n",
    "encoded_cols = [f'encoded_{i}' for i in range(encoded_features.shape[1])]\n",
    "    \n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_features, \n",
    "    columns=encoded_cols,\n",
    "    index=merged_data.index\n",
    ")\n",
    "\n",
    "# Prepare the final dataset\n",
    "X_numerical = merged_data[numerical_features]\n",
    "X = pd.concat([X_numerical.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "y = pd.to_numeric(merged_data['milliseconds'], errors='coerce')\n",
    "\n",
    "# Remove rows with NaN in target variable\n",
    "mask = ~y.isna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"Final dataset - X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Speed up by reducing dataset size if it's very large (optional)\n",
    "if X.shape[0] > 50000:\n",
    "    print(f\"Reducing dataset size from {X.shape[0]} to 50000 rows for faster training\")\n",
    "    random_indices = np.random.choice(X.shape[0], 50000, replace=False)\n",
    "    X = X.iloc[random_indices]\n",
    "    y = y.iloc[random_indices]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical = scaler.fit_transform(X_train.iloc[:, :len(numerical_features)])\n",
    "X_test_numerical = scaler.transform(X_test.iloc[:, :len(numerical_features)])\n",
    "X_train.iloc[:, :len(numerical_features)] = X_train_numerical\n",
    "X_test.iloc[:, :len(numerical_features)] = X_test_numerical\n",
    "\n",
    "# Create feature names list for artifacts (all as strings)\n",
    "feature_names = numerical_features.copy() + encoded_cols\n",
    "\n",
    "# Function to create and save artifacts - simplified to improve speed\n",
    "def create_artifacts(model, X_test, y_test, y_pred, feature_names, run_id):\n",
    "    # Feature importance plot - only plot top 10 features\n",
    "    feature_importance = pd.DataFrame(\n",
    "        model.feature_importances_,\n",
    "        index=feature_names,\n",
    "        columns=['importance']\n",
    "    ).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    feature_importance.head(10).plot(kind='bar')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feature_importance_{run_id}.png', dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save feature importance as CSV\n",
    "    feature_importance.to_csv(f'feature_importance_{run_id}.csv')\n",
    "    \n",
    "    # Residuals plot - sample fewer points for faster plotting\n",
    "    if len(y_pred) > 1000:\n",
    "        idx = np.random.choice(len(y_pred), 1000, replace=False)\n",
    "        y_pred_sample = y_pred[idx]\n",
    "        y_test_sample = y_test.iloc[idx]\n",
    "    else:\n",
    "        y_pred_sample = y_pred\n",
    "        y_test_sample = y_test\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(y_pred_sample, y_test_sample - y_pred_sample, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot (Sample)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'residuals_{run_id}.png', dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save predictions in a smaller sample for faster saving\n",
    "    if len(y_pred) > 5000:\n",
    "        idx = np.random.choice(len(y_pred), 5000, replace=False)\n",
    "        pd.DataFrame({\n",
    "            'actual': y_test.iloc[idx], \n",
    "            'predicted': y_pred[idx]\n",
    "        }).to_csv(f'predictions_{run_id}.csv')\n",
    "    else:\n",
    "        pd.DataFrame({\n",
    "            'actual': y_test, \n",
    "            'predicted': y_pred\n",
    "        }).to_csv(f'predictions_{run_id}.csv')\n",
    "    \n",
    "    return [\n",
    "        f'feature_importance_{run_id}.png',\n",
    "        f'feature_importance_{run_id}.csv',\n",
    "        f'residuals_{run_id}.png',\n",
    "        f'predictions_{run_id}.csv'\n",
    "    ]\n",
    "\n",
    "# Function to evaluate model and log metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return rmse, mae, r2, y_pred\n",
    "\n",
    "# Function to run a single experiment\n",
    "def run_experiment(run_id, max_depth, n_estimators, min_samples_split):\n",
    "    run_name = f\"Run {run_id + 1}\"\n",
    "    print(f\"\\nStarting {run_name} with parameters:\")\n",
    "    print(f\"  max_depth: {max_depth}\")\n",
    "    print(f\"  n_estimators: {n_estimators}\")\n",
    "    print(f\"  min_samples_split: {min_samples_split}\")\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "        mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        # Train model with warm_start=True to speed up sequential fits\n",
    "        print(\"  Training model...\")\n",
    "        model = RandomForestRegressor(\n",
    "            max_depth=max_depth,\n",
    "            n_estimators=n_estimators,\n",
    "            min_samples_split=min_samples_split,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,  # Use all cores for faster training\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(\"  Evaluating model...\")\n",
    "        rmse, mae, r2, y_pred = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        \n",
    "        # Create and log artifacts\n",
    "        print(\"  Creating artifacts...\")\n",
    "        artifact_paths = create_artifacts(model, X_test, y_test, y_pred, feature_names, run_id)\n",
    "        \n",
    "        # Log artifacts\n",
    "        for artifact_path in artifact_paths:\n",
    "            mlflow.log_artifact(artifact_path)\n",
    "        \n",
    "        # Log the model itself\n",
    "        mlflow.sklearn.log_model(model, \"random-forest-model\")\n",
    "        \n",
    "        print(f\"  Completed {run_name}\")\n",
    "        print(f\"  Metrics - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Clean up artifact files to save space\n",
    "        for artifact_path in artifact_paths:\n",
    "            if os.path.exists(artifact_path):\n",
    "                os.remove(artifact_path)\n",
    "                \n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2\n",
    "        }\n",
    "\n",
    "# Run experiments with different parameters\n",
    "print(\"\\n=== Starting MLflow experiment runs ===\\n\")\n",
    "\n",
    "# Parameters to try\n",
    "parameter_combinations = [\n",
    "    # max_depth, n_estimators, min_samples_split\n",
    "    (5, 100, 2),     # Run 1\n",
    "    (10, 300, 2),    # Run 2  \n",
    "    (15, 500, 2),    # Run 3\n",
    "    (20, 800, 2),    # Run 4\n",
    "    (None, 1000, 2), # Run 5\n",
    "    (10, 100, 5),    # Run 6\n",
    "    (10, 300, 10),   # Run 7\n",
    "    (15, 300, 5),    # Run 8\n",
    "    (20, 500, 5),    # Run 9\n",
    "    (15, 800, 10)    # Run 10\n",
    "]\n",
    "\n",
    "# Run experiments in sequence (more reliable in Databricks)\n",
    "results = []\n",
    "for run_id, (max_depth, n_estimators, min_samples_split) in enumerate(parameter_combinations):\n",
    "    result = run_experiment(run_id, max_depth, n_estimators, min_samples_split)\n",
    "    results.append(result)\n",
    "\n",
    "# Print summary of all runs\n",
    "print(\"\\n=== Summary of All Runs ===\\n\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(by='r2', ascending=False))\n",
    "\n",
    "print(\"\\n=== All experiment runs completed successfully ===\")\n",
    "print(\"You can now view the results in the MLflow UI\")\n",
    "print(\"Remember to take screenshots of:\")\n",
    "print(\"1. The MLflow homepage showing all your runs\")\n",
    "print(\"2. The detailed page of your best run\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Build a model with tunable hyperparameters",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}